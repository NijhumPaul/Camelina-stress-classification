{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f460df-e84e-4e73-9ae0-4fb14369b21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "import time\n",
    "import utils\n",
    "import random \n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "n=224\n",
    "image_size=(n, n)\n",
    "batch_size=16\n",
    "epochs=700\n",
    "models_name=\"custom\"\n",
    "\n",
    "TRAINING_DIR = 'split_2_5_25_split_70/train'\n",
    "VALIDATION_DIR = 'split_2_5_25_split_70/valid'\n",
    "Test_DIR = 'split_2_5_25_split_70/test'\n",
    "\n",
    "\n",
    "pre_in_densenet = tf.keras.applications.densenet.preprocess_input\n",
    "train_data_gen, valid_data_gen,test_data_gen, class_num = utils.load_data(image_size, batch_size, \n",
    "                                                TRAINING_DIR, VALIDATION_DIR, Test_DIR, pre_in_densenet)\n",
    "\n",
    "# Define the hybrid model with a reduced DenseNet backbone\n",
    "def create_hybrid_model(input_shape=(n, n, 3), num_classes=class_num):\n",
    "    \"\"\" Hybrid Model with Fewer DenseNet Layers + Transformer Block \"\"\"\n",
    "    \n",
    "    # Load DenseNet but stop at an earlier layer (shallower model)\n",
    "    densenet = DenseNet121(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Print all layer names to find the correct one\n",
    "    #for i, layer in enumerate(densenet.layers):\n",
    "    \t#print(i, layer.name)\n",
    "    # Reduce the number of layers (stop at an earlier stage)\n",
    "    densenet = Model(inputs=densenet.input, outputs=densenet.get_layer(\"conv2_block6_concat\").output)\n",
    "\n",
    "    # Input Layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = densenet(inputs)  # Shape: (batch_size, 14, 14, 896) instead of full 1024\n",
    "\n",
    "    # Global Pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Shape: (batch_size, 896)\n",
    "\n",
    "    # Ensure reshaping is compatible\n",
    "    num_patches = 16\n",
    "    projection_dim = x.shape[-1] // num_patches\n",
    "    if x.shape[-1] % num_patches != 0:\n",
    "        raise ValueError(f\"Cannot reshape tensor of shape {x.shape[-1]} into {num_patches} patches.\")\n",
    "    x = layers.Reshape((num_patches, projection_dim))(x)\n",
    "\n",
    "    # Apply Transformer Block (1 Layer)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.MultiHeadAttention(num_heads=8, key_dim=projection_dim)(x, x)\n",
    "\n",
    "    # Global Pooling\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # Classification Head\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = create_hybrid_model()\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model_name = model.name\n",
    "checkpoint, tb_callback=utils.logs(models_name)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=50,   # Stop if no improvement after 50 epochs\n",
    "    restore_best_weights=True\n",
    "    )\n",
    "\n",
    "t0 = time.time()\n",
    "history = model.fit(\n",
    "    train_data_gen,  # Your training data generator\n",
    "    validation_data=valid_data_gen,  # Your validation data generator\n",
    "    epochs=epochs,\n",
    "    callbacks=[tb_callback, checkpoint, early_stopping],  # Add callbacks (e.g., ModelCheckpoint, EarlyStopping)\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training time:\", time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d059f-45b9-42e5-9385-09b9ecf5ef1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.test_model(model, test_data_gen, models_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b1daa-8653-421c-9a5a-7ce9b14d7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history , models_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fd303-e905-49e4-be48-725901b8cacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1f22f-bbaa-4c62-82c1-724304e48b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (camelina-env)",
   "language": "python",
   "name": "camelina-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
